{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q4,Q5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VSwGTg93I9EX"},"source":["# Q4 - RBFNN"]},{"cell_type":"code","metadata":{"id":"9Z1qWt4-I7Gd","executionInfo":{"status":"ok","timestamp":1606838077845,"user_tz":-330,"elapsed":2880,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["import numpy as np\n","from scipy.io import loadmat\n","from sklearn.cluster import KMeans\n","\n","mat_contents = loadmat('/content/drive/MyDrive/2018A8PS1229H_NNFL/Data/data5.mat')\n","data = mat_contents['x']\n","np.random.shuffle(data)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"LN8KMpJXJIit","executionInfo":{"status":"ok","timestamp":1606838080764,"user_tz":-330,"elapsed":1280,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["def init_data():\n","    X = np.array(data[:2148, :-1], dtype = float)\n","    y = np.array(data[:2148, -1], dtype = int)\n","    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n","    return X, y\n","\n","def gaussian(x,center,sigma,beta):\n","    return np.exp(-beta * (np.linalg.norm(x - center)) ** 2)\n","\n","def multi_quadric(x, center, sigma, beta):\n","    return ((np.linalg.norm(x - center)) ** 2 + sigma ** 2) ** 0.5\n","\n","def linear(x, center, sigma, beta):\n","    return np.linalg.norm(x - center)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"xb615gLrJMTV","executionInfo":{"status":"ok","timestamp":1606838083643,"user_tz":-330,"elapsed":1208,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["X_tot, y_tot = init_data()\n","split_num = 358\n","train_X = X_tot[ : 1600]\n","train_y = y_tot[ : 1600]\n","test_X = X_tot[1600 : 2148]\n","test_y = y_tot[1600 : 2148]\n","\n","def fit_rbf(train_X, train_y, test_X, test_y):\n","    km = KMeans(n_clusters=550)\n","\n","    y_km = km.fit_predict(train_X)\n","    centers = km.cluster_centers_\n","    labels = km.predict(train_X)\n","\n","    sigma = np.zeros((len(centers), 1))\n","    beta = np.zeros((len(centers), 1))\n","    cluster_size = np.zeros((len(centers), 1))\n","\n","    for i in range(len(train_X)):\n","        sigma[labels[i]] += np.linalg.norm(train_X[i] - centers[labels[i]])\n","        cluster_size[labels[i]] += 1\n","\n","    sigma /= cluster_size\n","    beta = 1 / 2 * (sigma * sigma + 1e-6)\n","\n","    H = np.zeros((len(train_X), len(centers)))\n","\n","    for i in range(len(train_X)):\n","        for j in range(len(centers)):\n","            H[i, j] = linear(train_X[i], centers[j], sigma[j], beta[j])\n","\n","    W = np.dot(np.linalg.pinv(H), train_y)\n","\n","    #Test run\n","    H_test = np.zeros([len(test_X), len(centers)])\n","    for i in range(len(test_X)):\n","        for j in range(len(centers)):\n","            H_test[i, j] = linear(test_X[i], centers[j], sigma[j], beta[j])\n","\n","    y_pred = np.dot(H_test, W)\n","    for i in range(len(y_pred)):\n","        y_pred[i] = 1 if y_pred[i]>=0.5 else 0\n","        \n","    accuracy = 0    \n","    for i in range(len(y_pred)):\n","        if y_pred[i] == test_y[i]:\n","            accuracy +=1\n","    accuracy /= len(y_pred)\n","    print(accuracy)\n","    return y_pred, accuracy"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLc_k_2eJT4u","executionInfo":{"status":"ok","timestamp":1606838105208,"user_tz":-330,"elapsed":17395,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}},"outputId":"216426f9-35dc-4c1a-c034-7ba74d205c03"},"source":["y_pred, _ = fit_rbf(train_X, train_y, test_X, test_y)\n","for i in range(len(y_pred)):\n","    y_pred[i] = 1 if y_pred[i] > 0.5 else 0\n","\n","TP, TN, FP, FN = 0,0,0,0 \n","\n","for i in range(len(test_X)):\n","    \n","    if y_pred[i] == 1 and test_y[i] == 1:\n","        TP += 1\n","    \n","    elif y_pred[i] == 0 and test_y[i] == 0:\n","        TN += 1\n","        \n","    elif y_pred[i] == 1 and test_y[i] == 0:\n","        FP += 1\n","        \n","    elif y_pred[i] == 0 and test_y[i] == 1:\n","        FN += 1\n","        \n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)\n","print(TP, FP)\n","print(FN, TN)\n","avg_acc = 0"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0.958029197080292\n","accuracy =  0.958029197080292 sensitivity =  0.9688581314878892 specificity =  0.9459459459459459\n","280 14\n","9 245\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0YEGXuPJiol","executionInfo":{"status":"ok","timestamp":1606838220408,"user_tz":-330,"elapsed":83327,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}},"outputId":"555a1abb-66ed-4be0-a6c0-6a235d841a0e"},"source":["# K - fold cross validation\n","\n","for k in range(5):\n","    X = X_tot[0 : 1790]\n","    y = y_tot[0 : 1790]\n","    X_val = X_tot[1790 :]\n","    y_val = y_tot[1790 :]\n","    _, acc = fit_rbf(X, y, X_val, y_val)\n","    avg_acc += acc\n","    X_tot[: split_num] = X_val\n","    X_tot[split_num : ] = X\n","    y_tot[0 : split_num] = y_val\n","    y_tot[split_num : ] = y\n","\n","avg_acc /= 5\n","print(avg_acc)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.9497206703910615\n","0.9581005586592178\n","0.9692737430167597\n","0.9608938547486033\n","0.946927374301676\n","0.9569832402234637\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pVoQjvFILO5x"},"source":["# Q5 - Stacked Autoencoder"]},{"cell_type":"code","metadata":{"id":"4tfOopyNLVOP","executionInfo":{"status":"ok","timestamp":1606838758588,"user_tz":-330,"elapsed":1234,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["import numpy as np\n","from scipy.io import loadmat\n","from sklearn.preprocessing import normalize\n","\n","#Load data, shuffle and normalize\n","mat_contents = loadmat('/content/drive/MyDrive/2018A8PS1229H_NNFL/Data/data5.mat')\n","data = mat_contents['x']\n","np.random.shuffle(data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"nIJG0qeaLZH1","executionInfo":{"status":"ok","timestamp":1606838762432,"user_tz":-330,"elapsed":1188,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["def init_data():\n","    X = np.array(data[ : , :-1], dtype = float)\n","    y = np.array(data[ : , -1], dtype = int)\n","    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n","    return X, y\n","\n","X, y = init_data()\n","\n","#Hold out method of model evaluation\n","X_train, y_train = X[ :int(0.7 * len(X))], y[ :int(0.7 * len(X))]\n","X_val, y_val = X[ int(0.7 * len(X)): ], y[ int(0.7 * len(X)): ]\n","\n","alpha = 0.5\n","\n","#Sigmoid activation function\n","def sigmoid(x, derivative=False):\n","        if (derivative == True):\n","            return x * (1 - x)\n","        return 1 / (1 + np.exp(-x))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkaGYohLLe40","executionInfo":{"status":"ok","timestamp":1606838768820,"user_tz":-330,"elapsed":1205,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["class NeuralNetwork(object):\n","    def __init__(self, sizes):\n","        \n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.W = {}\n","        self.a = {}\n","        self.b = {}\n","        \n","        #Initialize Weights\n","        for i in range(1, self.num_layers):\n","            self.W[i] = np.random.randn(self.sizes[i-1], self.sizes[i])\n","            \n","        #Initialize biases\n","        for i in range(1, self.num_layers):\n","            self.b[i] = np.random.randn(self.sizes[i], 1)\n","        \n","        #Initialize activations\n","        for i in range(1, self.num_layers):\n","            self.a[i] = np.zeros([self.sizes[i], 1])\n","        \n","    #Forward pass to compute scores\n","    def forward_pass(self, X):\n","        \n","        self.a[0] = X\n","        \n","        for i in range(1, self.num_layers):\n","            self.a[i] = sigmoid(np.dot(self.W[i].T, self.a[i-1]) + self.b[i])\n","\n","        return self.a[self.num_layers-1] \n","    \n","    #Backward pass to update weights\n","    def backward_pass(self, X, Y, output):\n","        \n","        self.d = {}\n","        self.d_output = (Y - output) * sigmoid(output, derivative=True)\n","        self.d[self.num_layers-1] = self.d_output\n","        \n","        #Derivatives of the layers wrt loss\n","        for i in range(self.num_layers-1, 1, -1):\n","            self.d[i-1] = np.dot(self.W[i], self.d[i]) * sigmoid(self.a[i-1], derivative=True)\n","        \n","        #Updating weights\n","        for i in range(1, self.num_layers-1):\n","            self.W[i] += alpha * np.dot(self.a[i-1], self.d[i].T)\n","            \n","        #Updating biases\n","        for i in range(1, self.num_layers-1):\n","            self.b[i] += alpha * self.d[i]\n","\n","    #Training helper function   \n","    def train(self, X, Y):\n","        X = np.reshape(X, (len(X), 1))\n","        output = self.forward_pass(X)\n","        self.backward_pass(X, Y, output)\n","\n","    #Get weights    \n","    def get_W(self):\n","        return self.W\n","    \n","    #Load specified weights\n","    def load_W(self, W):\n","        self.W = W\n","\n","    #Scores computation for given input    \n","    def get_a(self, x):\n","        x = np.reshape(x, (len(x), 1))\n","        self.forward_pass(x)\n","        return self.a\n","    \n","    #Helper function for autoencoder chaining\n","    def load_a(self, a):\n","        self.a = a"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXRxM1lgLm13","executionInfo":{"status":"ok","timestamp":1606838773719,"user_tz":-330,"elapsed":1272,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}}},"source":["#Loss function\n","def calc_loss(NN,x ,y):\n","    \n","    loss = 0\n","    for i in range(len(x)):\n","        x_ = np.reshape(x[i], (len(x[i]), 1))\n","        loss += 0.5 / len(x) * np.sum((y[i] - NN.forward_pass(x_)) ** 2)\n","    \n","    return loss"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXFHy7SMLqOW","executionInfo":{"status":"ok","timestamp":1606838941938,"user_tz":-330,"elapsed":166262,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}},"outputId":"1ed50547-e5b2-4524-b5e4-3bdbb38fd833"},"source":["#Network initialization\n","autoencoder1 = NeuralNetwork([72, 60, 72])\n","autoencoder2 = NeuralNetwork([60,40,60])\n","autoencoder3 = NeuralNetwork([40, 30, 40])\n","NN = NeuralNetwork([72,60,40,30, 1])\n","\n","#Autoencoder 1 pretraining\n","for i in range(200):\n","    for j, row in enumerate(X_train):\n","        row = np.reshape(row, (72,1))\n","        autoencoder1.train(row, row)\n","        \n","    loss = calc_loss(autoencoder1, X_train, X_train)\n","    print(\"Epoch {}, Loss {}\".format(i, loss))\n","    \n","#Scores computation for autoencoder 1\n","autoencoder2_input = []\n","\n","for row in X_train:\n","    autoencoder2_input.append(autoencoder1.get_a(row)[1])\n","\n","autoencoder2_input = np.array(autoencoder2_input)\n","\n","\n","#Autoencoder 2 pretraining\n","for i in range(200):\n","    for j, row in enumerate(autoencoder2_input):\n","        row = np.reshape(row, (60,1))\n","        autoencoder2.train(row, row)\n","        \n","    loss = calc_loss(autoencoder2, autoencoder2_input, autoencoder2_input)\n","    print(\"Epoch {}, Loss {}\".format(i, loss))\n","\n","\n","#Scores computation for autoencoder 2\n","autoencoder3_input = []\n","\n","for row in autoencoder2_input:\n","    autoencoder3_input.append(autoencoder2.get_a(row)[1])\n","\n","autoencoder3_input = np.array(autoencoder3_input)\n","\n","#Autoencoder 3 pretraining\n","for i in range(200):\n","    for j, row in enumerate(autoencoder3_input):\n","        row = np.reshape(row, (40,1))\n","        autoencoder3.train(row, row)\n","        \n","    loss = calc_loss(autoencoder3, autoencoder3_input, autoencoder3_input)\n","    print(\"Epoch {}, Loss {}\".format(i, loss))\n","\n","#Final network weight initialization\n","W1 = autoencoder1.get_W()[1]\n","W2 = autoencoder2.get_W()[1]\n","W3 = autoencoder3.get_W()[1]\n","W_final = {}\n","W_final[1] = W1\n","W_final[2] = W2\n","W_final[3] = W3\n","W_final[4] = np.random.randn(30, 1)\n","NN.load_W(W_final)\n","\n","#Training loop\n","for i in range(500):\n","    print(\"Epoch: \", i)\n","\n","    for j in range(len(X_train)):\n","        NN.train(X_train[j], y_train[j])\n","\n","TP,TN,FP,FN = 0,0,0,0\n","\n","for i in range(len(X_val)):\n","\n","    x = np.reshape(X_val[i], (len(X_val[i]), 1))\n","    x = NN.forward_pass(x)\n","    p = 0 if x[0] < 0.5 else 1\n","\n","    if p == 1 and y_val[i] == 1:\n","        TP += 1\n","    elif p == 0 and y_val[i] == 0:\n","        TN += 1\n","    elif p == 1 and y_val[i] == 0:\n","        FP += 1\n","    elif p == 0 and y_val[i] == 1:\n","        FN += 1\n","\n","print(TP, FP)\n","print(FN, TN)\n","\n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0, Loss 3111.9183457398008\n","Epoch 1, Loss 3073.356819501463\n","Epoch 2, Loss 3055.3164064178104\n","Epoch 3, Loss 3043.6610746389683\n","Epoch 4, Loss 3027.6370621306073\n","Epoch 5, Loss 3033.9044169372537\n","Epoch 6, Loss 3031.119042934751\n","Epoch 7, Loss 3024.6214952993078\n","Epoch 8, Loss 3026.1328658236935\n","Epoch 9, Loss 3020.918916457581\n","Epoch 10, Loss 3019.3618279588263\n","Epoch 11, Loss 3017.5086773918356\n","Epoch 12, Loss 3019.3579187095984\n","Epoch 13, Loss 3016.5892190301192\n","Epoch 14, Loss 3024.7139394218616\n","Epoch 15, Loss 3021.228438967541\n","Epoch 16, Loss 3023.45836490839\n","Epoch 17, Loss 3021.706364820135\n","Epoch 18, Loss 3018.71839333829\n","Epoch 19, Loss 3019.8763625720585\n","Epoch 20, Loss 3017.110749625585\n","Epoch 21, Loss 3021.1345664197765\n","Epoch 22, Loss 3017.965975351277\n","Epoch 23, Loss 3023.398653109415\n","Epoch 24, Loss 3019.99152442701\n","Epoch 25, Loss 3017.772690291936\n","Epoch 26, Loss 3021.323439296316\n","Epoch 27, Loss 3013.714590338511\n","Epoch 28, Loss 3017.9868374327543\n","Epoch 29, Loss 3015.98503621487\n","Epoch 30, Loss 3017.4888813847497\n","Epoch 31, Loss 3030.5816298905506\n","Epoch 32, Loss 3018.661713945197\n","Epoch 33, Loss 3017.1049822542373\n","Epoch 34, Loss 3016.506976619861\n","Epoch 35, Loss 3012.085471896933\n","Epoch 36, Loss 3014.575610717376\n","Epoch 37, Loss 3018.704996632008\n","Epoch 38, Loss 3015.1231296437886\n","Epoch 39, Loss 3016.6172234969076\n","Epoch 40, Loss 3013.472837403048\n","Epoch 41, Loss 3014.7374266739844\n","Epoch 42, Loss 3014.1585883482953\n","Epoch 43, Loss 3025.8083413713753\n","Epoch 44, Loss 3020.7904283821563\n","Epoch 45, Loss 3013.8740404439823\n","Epoch 46, Loss 3014.423221010382\n","Epoch 47, Loss 3015.9331331120256\n","Epoch 48, Loss 3013.822423862027\n","Epoch 49, Loss 3013.6675613775255\n","Epoch 50, Loss 3015.946789626087\n","Epoch 51, Loss 3019.4016503745247\n","Epoch 52, Loss 3017.398019868342\n","Epoch 53, Loss 3016.0110277754975\n","Epoch 54, Loss 3017.4811005541983\n","Epoch 55, Loss 3014.661167625904\n","Epoch 56, Loss 3015.2500581985123\n","Epoch 57, Loss 3014.561325931186\n","Epoch 58, Loss 3014.3436157967217\n","Epoch 59, Loss 3014.9929591505315\n","Epoch 60, Loss 3011.2183293421735\n","Epoch 61, Loss 3013.720678984435\n","Epoch 62, Loss 3014.0886892853146\n","Epoch 63, Loss 3010.7298484712587\n","Epoch 64, Loss 3010.3756387871435\n","Epoch 65, Loss 3011.1433168555122\n","Epoch 66, Loss 3011.410733458544\n","Epoch 67, Loss 3013.6648181842747\n","Epoch 68, Loss 3012.76612497055\n","Epoch 69, Loss 3013.450347925398\n","Epoch 70, Loss 3015.412254939713\n","Epoch 71, Loss 3014.7218717731307\n","Epoch 72, Loss 3013.1739197537277\n","Epoch 73, Loss 3014.5602165403366\n","Epoch 74, Loss 3010.9041036304784\n","Epoch 75, Loss 3011.783681875601\n","Epoch 76, Loss 3011.9253387172103\n","Epoch 77, Loss 3011.3149979929162\n","Epoch 78, Loss 3012.8313664547068\n","Epoch 79, Loss 3011.888651328188\n","Epoch 80, Loss 3011.1478951220943\n","Epoch 81, Loss 3012.470332165059\n","Epoch 82, Loss 3012.6420833816874\n","Epoch 83, Loss 3015.1029783231234\n","Epoch 84, Loss 3013.0363646557676\n","Epoch 85, Loss 3012.876055843829\n","Epoch 86, Loss 3016.1510347562435\n","Epoch 87, Loss 3013.654045906161\n","Epoch 88, Loss 3015.5259204791932\n","Epoch 89, Loss 3016.812440056923\n","Epoch 90, Loss 3015.5048702118233\n","Epoch 91, Loss 3016.8103485288416\n","Epoch 92, Loss 3015.939758706145\n","Epoch 93, Loss 3018.0443323426234\n","Epoch 94, Loss 3016.171790518887\n","Epoch 95, Loss 3013.1202665120704\n","Epoch 96, Loss 3015.215962957437\n","Epoch 97, Loss 3014.1727026699095\n","Epoch 98, Loss 3013.7004465535706\n","Epoch 99, Loss 3014.785052028406\n","Epoch 100, Loss 3013.3877570354166\n","Epoch 101, Loss 3013.40011477274\n","Epoch 102, Loss 3012.507181592778\n","Epoch 103, Loss 3014.9375236866235\n","Epoch 104, Loss 3013.8292688146553\n","Epoch 105, Loss 3012.28013104059\n","Epoch 106, Loss 3012.191318869012\n","Epoch 107, Loss 3010.6382762365643\n","Epoch 108, Loss 3013.0395277725847\n","Epoch 109, Loss 3013.6166325405084\n","Epoch 110, Loss 3013.1306901361013\n","Epoch 111, Loss 3014.2504761729147\n","Epoch 112, Loss 3011.980826098446\n","Epoch 113, Loss 3011.241260912852\n","Epoch 114, Loss 3012.049995162432\n","Epoch 115, Loss 3013.1516278959184\n","Epoch 116, Loss 3010.6205938129747\n","Epoch 117, Loss 3011.6302281190556\n","Epoch 118, Loss 3013.5526856813053\n","Epoch 119, Loss 3011.4957017650336\n","Epoch 120, Loss 3012.4326680316367\n","Epoch 121, Loss 3013.1407481513293\n","Epoch 122, Loss 3014.2115728673953\n","Epoch 123, Loss 3012.1438914168043\n","Epoch 124, Loss 3011.711105516342\n","Epoch 125, Loss 3014.187838436681\n","Epoch 126, Loss 3010.1198312959327\n","Epoch 127, Loss 3013.7398764903774\n","Epoch 128, Loss 3012.3663579807862\n","Epoch 129, Loss 3013.29677721347\n","Epoch 130, Loss 3013.4801115840264\n","Epoch 131, Loss 3011.0640085356467\n","Epoch 132, Loss 3010.8058184019064\n","Epoch 133, Loss 3012.1064998010893\n","Epoch 134, Loss 3011.265604111828\n","Epoch 135, Loss 3012.622046412974\n","Epoch 136, Loss 3012.632671064412\n","Epoch 137, Loss 3010.6460291962712\n","Epoch 138, Loss 3011.038448695437\n","Epoch 139, Loss 3012.011607023431\n","Epoch 140, Loss 3011.405600074911\n","Epoch 141, Loss 3011.8581655660714\n","Epoch 142, Loss 3011.093877864985\n","Epoch 143, Loss 3013.3839676672324\n","Epoch 144, Loss 3012.300461872334\n","Epoch 145, Loss 3012.4650846482314\n","Epoch 146, Loss 3011.663162025548\n","Epoch 147, Loss 3012.559433917235\n","Epoch 148, Loss 3011.9672209825085\n","Epoch 149, Loss 3010.3072170486516\n","Epoch 150, Loss 3011.8170041842563\n","Epoch 151, Loss 3013.292902165596\n","Epoch 152, Loss 3014.114768905591\n","Epoch 153, Loss 3011.983280862809\n","Epoch 154, Loss 3012.0383373889586\n","Epoch 155, Loss 3011.3374695633465\n","Epoch 156, Loss 3011.3635516143454\n","Epoch 157, Loss 3009.7875599618774\n","Epoch 158, Loss 3010.9129746848052\n","Epoch 159, Loss 3010.6067404287523\n","Epoch 160, Loss 3009.3562104780194\n","Epoch 161, Loss 3011.1169592581114\n","Epoch 162, Loss 3009.745256929517\n","Epoch 163, Loss 3012.0492281772194\n","Epoch 164, Loss 3011.695074651776\n","Epoch 165, Loss 3010.250224539089\n","Epoch 166, Loss 3010.9159297299398\n","Epoch 167, Loss 3011.3728467928213\n","Epoch 168, Loss 3012.989159384791\n","Epoch 169, Loss 3009.820540161005\n","Epoch 170, Loss 3010.6502269255975\n","Epoch 171, Loss 3010.018433301985\n","Epoch 172, Loss 3010.730200849409\n","Epoch 173, Loss 3010.093185505902\n","Epoch 174, Loss 3011.126852660651\n","Epoch 175, Loss 3013.587498629274\n","Epoch 176, Loss 3011.5699069822876\n","Epoch 177, Loss 3010.6440875792923\n","Epoch 178, Loss 3010.854609921482\n","Epoch 179, Loss 3009.862218748282\n","Epoch 180, Loss 3010.3798413622\n","Epoch 181, Loss 3011.046798124756\n","Epoch 182, Loss 3011.8198732776873\n","Epoch 183, Loss 3011.407966312953\n","Epoch 184, Loss 3010.919106394483\n","Epoch 185, Loss 3011.3007499371593\n","Epoch 186, Loss 3011.597569309601\n","Epoch 187, Loss 3012.993013699773\n","Epoch 188, Loss 3009.535457908263\n","Epoch 189, Loss 3010.2250171870164\n","Epoch 190, Loss 3010.2175074165875\n","Epoch 191, Loss 3013.0540763398712\n","Epoch 192, Loss 3014.0817681941617\n","Epoch 193, Loss 3013.227371705691\n","Epoch 194, Loss 3010.9183724959184\n","Epoch 195, Loss 3010.344073383554\n","Epoch 196, Loss 3012.0231731862896\n","Epoch 197, Loss 3011.2347328715673\n","Epoch 198, Loss 3009.4861605093133\n","Epoch 199, Loss 3011.5321794887477\n","Epoch 0, Loss 5.809193193043704\n","Epoch 1, Loss 5.55993262131534\n","Epoch 2, Loss 5.434889292551532\n","Epoch 3, Loss 5.356539014982476\n","Epoch 4, Loss 5.273945515542788\n","Epoch 5, Loss 5.235156552681217\n","Epoch 6, Loss 5.210933907453017\n","Epoch 7, Loss 5.192520771424796\n","Epoch 8, Loss 5.182061900602755\n","Epoch 9, Loss 5.171720906741404\n","Epoch 10, Loss 5.1633112808624695\n","Epoch 11, Loss 5.158283910304937\n","Epoch 12, Loss 5.15325112890004\n","Epoch 13, Loss 5.145271680419953\n","Epoch 14, Loss 5.139829799775078\n","Epoch 15, Loss 5.137699976420101\n","Epoch 16, Loss 5.134180253301149\n","Epoch 17, Loss 5.131328829288888\n","Epoch 18, Loss 5.129735320955224\n","Epoch 19, Loss 5.1286969260562865\n","Epoch 20, Loss 5.12791662175238\n","Epoch 21, Loss 5.127358458517913\n","Epoch 22, Loss 5.126973662452914\n","Epoch 23, Loss 5.126665132339275\n","Epoch 24, Loss 5.126293578712996\n","Epoch 25, Loss 5.125704422916963\n","Epoch 26, Loss 5.124796479568348\n","Epoch 27, Loss 5.12234032618145\n","Epoch 28, Loss 5.118666836668433\n","Epoch 29, Loss 5.1125200228146275\n","Epoch 30, Loss 5.108447385162896\n","Epoch 31, Loss 5.106692207053741\n","Epoch 32, Loss 5.104963880790301\n","Epoch 33, Loss 5.103973968908232\n","Epoch 34, Loss 5.103526222113571\n","Epoch 35, Loss 5.103212097012228\n","Epoch 36, Loss 5.102848682339683\n","Epoch 37, Loss 5.102563616197139\n","Epoch 38, Loss 5.102184999478366\n","Epoch 39, Loss 5.101496588807771\n","Epoch 40, Loss 5.100587525521115\n","Epoch 41, Loss 5.099734062273558\n","Epoch 42, Loss 5.099118547277409\n","Epoch 43, Loss 5.098678827835576\n","Epoch 44, Loss 5.098321871505114\n","Epoch 45, Loss 5.098009309321634\n","Epoch 46, Loss 5.0977147567946925\n","Epoch 47, Loss 5.097407404774899\n","Epoch 48, Loss 5.097028204543178\n","Epoch 49, Loss 5.096373567797554\n","Epoch 50, Loss 5.095139925886873\n","Epoch 51, Loss 5.094598952906652\n","Epoch 52, Loss 5.093901598935849\n","Epoch 53, Loss 5.093524376836294\n","Epoch 54, Loss 5.093521697544648\n","Epoch 55, Loss 5.093464616489345\n","Epoch 56, Loss 5.09346142407071\n","Epoch 57, Loss 5.09360118385288\n","Epoch 58, Loss 5.0937737617633\n","Epoch 59, Loss 5.093850657968921\n","Epoch 60, Loss 5.0936524897626425\n","Epoch 61, Loss 5.093137598712787\n","Epoch 62, Loss 5.092521817457129\n","Epoch 63, Loss 5.09187464437924\n","Epoch 64, Loss 5.091225338505272\n","Epoch 65, Loss 5.090529804708534\n","Epoch 66, Loss 5.0897667940596145\n","Epoch 67, Loss 5.089017092484587\n","Epoch 68, Loss 5.0883933283713985\n","Epoch 69, Loss 5.088076047221882\n","Epoch 70, Loss 5.087851495752078\n","Epoch 71, Loss 5.087481238051344\n","Epoch 72, Loss 5.087106297409\n","Epoch 73, Loss 5.08680197178384\n","Epoch 74, Loss 5.086490843981827\n","Epoch 75, Loss 5.086179060211055\n","Epoch 76, Loss 5.085891732860793\n","Epoch 77, Loss 5.085661892954729\n","Epoch 78, Loss 5.085537330407216\n","Epoch 79, Loss 5.0855113870324855\n","Epoch 80, Loss 5.085538205623343\n","Epoch 81, Loss 5.0856029798402655\n","Epoch 82, Loss 5.0857070920409875\n","Epoch 83, Loss 5.085845432903262\n","Epoch 84, Loss 5.086008825412917\n","Epoch 85, Loss 5.086187533008412\n","Epoch 86, Loss 5.086372363694705\n","Epoch 87, Loss 5.086555399143366\n","Epoch 88, Loss 5.086730895414718\n","Epoch 89, Loss 5.086895732167683\n","Epoch 90, Loss 5.087048972396584\n","Epoch 91, Loss 5.087190858478867\n","Epoch 92, Loss 5.087321812771776\n","Epoch 93, Loss 5.087441739065188\n","Epoch 94, Loss 5.08754978788624\n","Epoch 95, Loss 5.087644870359529\n","Epoch 96, Loss 5.087726852785753\n","Epoch 97, Loss 5.087797492631304\n","Epoch 98, Loss 5.087860227895632\n","Epoch 99, Loss 5.087918446694356\n","Epoch 100, Loss 5.087972628164173\n","Epoch 101, Loss 5.088020024822893\n","Epoch 102, Loss 5.0880595754436095\n","Epoch 103, Loss 5.088095496254055\n","Epoch 104, Loss 5.088133669582079\n","Epoch 105, Loss 5.088176368977086\n","Epoch 106, Loss 5.088222019032832\n","Epoch 107, Loss 5.088267656362116\n","Epoch 108, Loss 5.0883105656921686\n","Epoch 109, Loss 5.088348740723588\n","Epoch 110, Loss 5.088380772001249\n","Epoch 111, Loss 5.088405535803588\n","Epoch 112, Loss 5.088421771444603\n","Epoch 113, Loss 5.088427377411317\n","Epoch 114, Loss 5.0884177097622345\n","Epoch 115, Loss 5.088380062134672\n","Epoch 116, Loss 5.0882739923346385\n","Epoch 117, Loss 5.088018660803918\n","Epoch 118, Loss 5.0876706298653716\n","Epoch 119, Loss 5.087366821396955\n","Epoch 120, Loss 5.087142615976699\n","Epoch 121, Loss 5.086942491407417\n","Epoch 122, Loss 5.086763204701683\n","Epoch 123, Loss 5.086601396744359\n","Epoch 124, Loss 5.086446692292524\n","Epoch 125, Loss 5.0862923060992955\n","Epoch 126, Loss 5.086103269139516\n","Epoch 127, Loss 5.085905362692165\n","Epoch 128, Loss 5.085740556720985\n","Epoch 129, Loss 5.085603500417463\n","Epoch 130, Loss 5.085450027605907\n","Epoch 131, Loss 5.083531420358639\n","Epoch 132, Loss 5.083577216808789\n","Epoch 133, Loss 5.082860036819362\n","Epoch 134, Loss 5.082045911145348\n","Epoch 135, Loss 5.081453039025585\n","Epoch 136, Loss 5.081131705873236\n","Epoch 137, Loss 5.080916597496089\n","Epoch 138, Loss 5.080499459473068\n","Epoch 139, Loss 5.079165049393067\n","Epoch 140, Loss 5.076765191176181\n","Epoch 141, Loss 5.075813465773464\n","Epoch 142, Loss 5.075544190731548\n","Epoch 143, Loss 5.075406648619652\n","Epoch 144, Loss 5.07527627251064\n","Epoch 145, Loss 5.075168502754167\n","Epoch 146, Loss 5.075157719176288\n","Epoch 147, Loss 5.0752893834112776\n","Epoch 148, Loss 5.075434942594049\n","Epoch 149, Loss 5.075568357008715\n","Epoch 150, Loss 5.075699531196888\n","Epoch 151, Loss 5.075826361946223\n","Epoch 152, Loss 5.0759308979040405\n","Epoch 153, Loss 5.076012393871218\n","Epoch 154, Loss 5.076094160423238\n","Epoch 155, Loss 5.07620741823624\n","Epoch 156, Loss 5.076364195063267\n","Epoch 157, Loss 5.076549173331056\n","Epoch 158, Loss 5.076737588737461\n","Epoch 159, Loss 5.076912760769829\n","Epoch 160, Loss 5.077069216122928\n","Epoch 161, Loss 5.0772077079772915\n","Epoch 162, Loss 5.077331686214185\n","Epoch 163, Loss 5.077448650121888\n","Epoch 164, Loss 5.077568835771759\n","Epoch 165, Loss 5.077695774715341\n","Epoch 166, Loss 5.077825487128042\n","Epoch 167, Loss 5.077955586033127\n","Epoch 168, Loss 5.078087716333307\n","Epoch 169, Loss 5.078224262914369\n","Epoch 170, Loss 5.078365922230132\n","Epoch 171, Loss 5.078511051489591\n","Epoch 172, Loss 5.078655245267837\n","Epoch 173, Loss 5.078790653873759\n","Epoch 174, Loss 5.078905585568824\n","Epoch 175, Loss 5.078981673833276\n","Epoch 176, Loss 5.078996162629041\n","Epoch 177, Loss 5.078952671626805\n","Epoch 178, Loss 5.078799973242676\n","Epoch 179, Loss 5.077717430104852\n","Epoch 180, Loss 5.074467940651865\n","Epoch 181, Loss 5.0708010453468315\n","Epoch 182, Loss 5.068623886952312\n","Epoch 183, Loss 5.065716558186843\n","Epoch 184, Loss 5.062865830128845\n","Epoch 185, Loss 5.060504351035155\n","Epoch 186, Loss 5.058931751734779\n","Epoch 187, Loss 5.058105055477036\n","Epoch 188, Loss 5.05773614392036\n","Epoch 189, Loss 5.057625934652281\n","Epoch 190, Loss 5.0576817761908455\n","Epoch 191, Loss 5.057859652094643\n","Epoch 192, Loss 5.058121999756312\n","Epoch 193, Loss 5.058432104387036\n","Epoch 194, Loss 5.058759580728361\n","Epoch 195, Loss 5.059083840114554\n","Epoch 196, Loss 5.0593963705500835\n","Epoch 197, Loss 5.059698344382933\n","Epoch 198, Loss 5.0600068719427\n","Epoch 199, Loss 5.060325477024671\n","Epoch 0, Loss 3.3235784084509223\n","Epoch 1, Loss 3.1652598518122534\n","Epoch 2, Loss 3.1408596703736182\n","Epoch 3, Loss 3.1213159878621513\n","Epoch 4, Loss 3.095476759543054\n","Epoch 5, Loss 3.0840891849473837\n","Epoch 6, Loss 3.0786181518423192\n","Epoch 7, Loss 3.075237173518231\n","Epoch 8, Loss 3.072909324111274\n","Epoch 9, Loss 3.0710830364649424\n","Epoch 10, Loss 3.0694112038828347\n","Epoch 11, Loss 3.068188725000941\n","Epoch 12, Loss 3.067431249693673\n","Epoch 13, Loss 3.0669051547201325\n","Epoch 14, Loss 3.066504490715198\n","Epoch 15, Loss 3.0661800660303444\n","Epoch 16, Loss 3.065898830846041\n","Epoch 17, Loss 3.0656158836253278\n","Epoch 18, Loss 3.064940681198716\n","Epoch 19, Loss 3.060753568537523\n","Epoch 20, Loss 3.0583501319650006\n","Epoch 21, Loss 3.056307866133953\n","Epoch 22, Loss 3.0544429350741074\n","Epoch 23, Loss 3.0519646167691943\n","Epoch 24, Loss 3.0502051854314454\n","Epoch 25, Loss 3.0485999229481755\n","Epoch 26, Loss 3.047447196837932\n","Epoch 27, Loss 3.0465713101894765\n","Epoch 28, Loss 3.045892262300662\n","Epoch 29, Loss 3.045365526950826\n","Epoch 30, Loss 3.0449507345511218\n","Epoch 31, Loss 3.044614205612766\n","Epoch 32, Loss 3.0443321331402124\n","Epoch 33, Loss 3.0440892565171795\n","Epoch 34, Loss 3.0438761303156827\n","Epoch 35, Loss 3.0436868289251473\n","Epoch 36, Loss 3.0435174468310966\n","Epoch 37, Loss 3.0433652226262793\n","Epoch 38, Loss 3.0432280539333374\n","Epoch 39, Loss 3.0431042353565467\n","Epoch 40, Loss 3.04299231846653\n","Epoch 41, Loss 3.0428910371475544\n","Epoch 42, Loss 3.042799267272447\n","Epoch 43, Loss 3.0427160038897365\n","Epoch 44, Loss 3.0426403469679326\n","Epoch 45, Loss 3.0425714910951944\n","Epoch 46, Loss 3.042508716912638\n","Epoch 47, Loss 3.042451383314734\n","Epoch 48, Loss 3.042398920071849\n","Epoch 49, Loss 3.042350820808202\n","Epoch 50, Loss 3.0423066363701747\n","Epoch 51, Loss 3.0422659686400855\n","Epoch 52, Loss 3.0422284648375086\n","Epoch 53, Loss 3.0421938123278935\n","Epoch 54, Loss 3.042161733937705\n","Epoch 55, Loss 3.0421319837585754\n","Epoch 56, Loss 3.042104343412667\n","Epoch 57, Loss 3.0420786187444264\n","Epoch 58, Loss 3.042054636901354\n","Epoch 59, Loss 3.0420322437655782\n","Epoch 60, Loss 3.042011301699237\n","Epoch 61, Loss 3.0419916875686264\n","Epoch 62, Loss 3.0419732910148465\n","Epoch 63, Loss 3.041956012941344\n","Epoch 64, Loss 3.0419397641917505\n","Epoch 65, Loss 3.041924464394404\n","Epoch 66, Loss 3.0419100409521813\n","Epoch 67, Loss 3.041896428159352\n","Epoch 68, Loss 3.0418835664287682\n","Epoch 69, Loss 3.041871401615307\n","Epoch 70, Loss 3.0418598844228386\n","Epoch 71, Loss 3.041848969883996\n","Epoch 72, Loss 3.0418386169030445\n","Epoch 73, Loss 3.0418287878538197\n","Epoch 74, Loss 3.0418194482254437\n","Epoch 75, Loss 3.0418105663097825\n","Epoch 76, Loss 3.0418021129250334\n","Epoch 77, Loss 3.041794061171134\n","Epoch 78, Loss 3.041786386212689\n","Epoch 79, Loss 3.041779065086157\n","Epoch 80, Loss 3.04177207652811\n","Epoch 81, Loss 3.041765400822122\n","Epoch 82, Loss 3.041759019661894\n","Epoch 83, Loss 3.0417529160288086\n","Epoch 84, Loss 3.0417470740819894\n","Epoch 85, Loss 3.0417414790595996\n","Epoch 86, Loss 3.0417361171899935\n","Epoch 87, Loss 3.041730975611581\n","Epoch 88, Loss 3.04172604230046\n","Epoch 89, Loss 3.041721306004902\n","Epoch 90, Loss 3.0417167561860556\n","Epoch 91, Loss 3.041712382964\n","Epoch 92, Loss 3.0417081770688017\n","Epoch 93, Loss 3.041704129795907\n","Epoch 94, Loss 3.041700232965447\n","Epoch 95, Loss 3.0416964788850445\n","Epoch 96, Loss 3.0416928603158526\n","Epoch 97, Loss 3.0416893704413934\n","Epoch 98, Loss 3.0416860028389885\n","Epoch 99, Loss 3.041682751453509\n","Epoch 100, Loss 3.0416796105732185\n","Epoch 101, Loss 3.0416765748075685\n","Epoch 102, Loss 3.0416736390666275\n","Epoch 103, Loss 3.041670798542202\n","Epoch 104, Loss 3.041668048690252\n","Epoch 105, Loss 3.0416653852147184\n","Epoch 106, Loss 3.0416628040524536\n","Epoch 107, Loss 3.0416603013593124\n","Epoch 108, Loss 3.0416578734971362\n","Epoch 109, Loss 3.0416555170217143\n","Epoch 110, Loss 3.041653228671553\n","Epoch 111, Loss 3.04165100535741\n","Epoch 112, Loss 3.0416488441524727\n","Epoch 113, Loss 3.0416467422832882\n","Epoch 114, Loss 3.041644697121172\n","Epoch 115, Loss 3.041642706174232\n","Epoch 116, Loss 3.0416407670798447\n","Epoch 117, Loss 3.0416388775976455\n","Epoch 118, Loss 3.041637035602891\n","Epoch 119, Loss 3.0416352390802768\n","Epoch 120, Loss 3.0416334861180805\n","Epoch 121, Loss 3.041631774902648\n","Epoch 122, Loss 3.0416301037131963\n","Epoch 123, Loss 3.0416284709169594\n","Epoch 124, Loss 3.0416268749645106\n","Epoch 125, Loss 3.041625314385422\n","Epoch 126, Loss 3.0416237877841428\n","Epoch 127, Loss 3.0416222938360717\n","Epoch 128, Loss 3.0416208312838426\n","Epoch 129, Loss 3.041619398933854\n","Epoch 130, Loss 3.041617995652894\n","Epoch 131, Loss 3.0416166203650277\n","Epoch 132, Loss 3.041615272048555\n","Epoch 133, Loss 3.0416139497332\n","Epoch 134, Loss 3.0416126524973808\n","Epoch 135, Loss 3.0416113794656616\n","Epoch 136, Loss 3.0416101298062466\n","Epoch 137, Loss 3.0416089027287474\n","Epoch 138, Loss 3.041607697481895\n","Epoch 139, Loss 3.04160651335144\n","Epoch 140, Loss 3.0416053496581714\n","Epoch 141, Loss 3.0416042057560118\n","Epoch 142, Loss 3.0416030810301518\n","Epoch 143, Loss 3.0416019748953596\n","Epoch 144, Loss 3.041600886794329\n","Epoch 145, Loss 3.041599816196112\n","Epoch 146, Loss 3.0415987625946115\n","Epoch 147, Loss 3.041597725507163\n","Epoch 148, Loss 3.0415967044732155\n","Epoch 149, Loss 3.041595699053015\n","Epoch 150, Loss 3.04159470882639\n","Epoch 151, Loss 3.0415937333915983\n","Epoch 152, Loss 3.0415927723642517\n","Epoch 153, Loss 3.0415918253762264\n","Epoch 154, Loss 3.041590892074729\n","Epoch 155, Loss 3.041589972121303\n","Epoch 156, Loss 3.041589065190984\n","Epoch 157, Loss 3.0415881709714645\n","Epoch 158, Loss 3.0415872891622686\n","Epoch 159, Loss 3.041586419474033\n","Epoch 160, Loss 3.0415855616277687\n","Epoch 161, Loss 3.0415847153542765\n","Epoch 162, Loss 3.041583880393411\n","Epoch 163, Loss 3.0415830564935717\n","Epoch 164, Loss 3.0415822434111437\n","Epoch 165, Loss 3.0415814409099413\n","Epoch 166, Loss 3.0415806487607555\n","Epoch 167, Loss 3.0415798667409253\n","Epoch 168, Loss 3.0415790946338386\n","Epoch 169, Loss 3.0415783322286316\n","Epoch 170, Loss 3.0415775793197484\n","Epoch 171, Loss 3.0415768357066395\n","Epoch 172, Loss 3.041576101193444\n","Epoch 173, Loss 3.0415753755886614\n","Epoch 174, Loss 3.041574658704935\n","Epoch 175, Loss 3.0415739503587833\n","Epoch 176, Loss 3.0415732503703143\n","Epoch 177, Loss 3.041572558563128\n","Epoch 178, Loss 3.0415718747640135\n","Epoch 179, Loss 3.041571198802835\n","Epoch 180, Loss 3.041570530512358\n","Epoch 181, Loss 3.0415698697280806\n","Epoch 182, Loss 3.041569216288125\n","Epoch 183, Loss 3.04156857003312\n","Epoch 184, Loss 3.0415679308060706\n","Epoch 185, Loss 3.04156729845226\n","Epoch 186, Loss 3.0415666728191835\n","Epoch 187, Loss 3.0415660537564695\n","Epoch 188, Loss 3.041565441115774\n","Epoch 189, Loss 3.041564834750759\n","Epoch 190, Loss 3.0415642345170215\n","Epoch 191, Loss 3.0415636402720336\n","Epoch 192, Loss 3.041563051875126\n","Epoch 193, Loss 3.04156246918743\n","Epoch 194, Loss 3.041561892071854\n","Epoch 195, Loss 3.0415613203930674\n","Epoch 196, Loss 3.0415607540174348\n","Epoch 197, Loss 3.0415601928130607\n","Epoch 198, Loss 3.041559636649716\n","Epoch 199, Loss 3.0415590853988603\n","Epoch:  0\n","Epoch:  1\n","Epoch:  2\n","Epoch:  3\n","Epoch:  4\n","Epoch:  5\n","Epoch:  6\n","Epoch:  7\n","Epoch:  8\n","Epoch:  9\n","Epoch:  10\n","Epoch:  11\n","Epoch:  12\n","Epoch:  13\n","Epoch:  14\n","Epoch:  15\n","Epoch:  16\n","Epoch:  17\n","Epoch:  18\n","Epoch:  19\n","Epoch:  20\n","Epoch:  21\n","Epoch:  22\n","Epoch:  23\n","Epoch:  24\n","Epoch:  25\n","Epoch:  26\n","Epoch:  27\n","Epoch:  28\n","Epoch:  29\n","Epoch:  30\n","Epoch:  31\n","Epoch:  32\n","Epoch:  33\n","Epoch:  34\n","Epoch:  35\n","Epoch:  36\n","Epoch:  37\n","Epoch:  38\n","Epoch:  39\n","Epoch:  40\n","Epoch:  41\n","Epoch:  42\n","Epoch:  43\n","Epoch:  44\n","Epoch:  45\n","Epoch:  46\n","Epoch:  47\n","Epoch:  48\n","Epoch:  49\n","Epoch:  50\n","Epoch:  51\n","Epoch:  52\n","Epoch:  53\n","Epoch:  54\n","Epoch:  55\n","Epoch:  56\n","Epoch:  57\n","Epoch:  58\n","Epoch:  59\n","Epoch:  60\n","Epoch:  61\n","Epoch:  62\n","Epoch:  63\n","Epoch:  64\n","Epoch:  65\n","Epoch:  66\n","Epoch:  67\n","Epoch:  68\n","Epoch:  69\n","Epoch:  70\n","Epoch:  71\n","Epoch:  72\n","Epoch:  73\n","Epoch:  74\n","Epoch:  75\n","Epoch:  76\n","Epoch:  77\n","Epoch:  78\n","Epoch:  79\n","Epoch:  80\n","Epoch:  81\n","Epoch:  82\n","Epoch:  83\n","Epoch:  84\n","Epoch:  85\n","Epoch:  86\n","Epoch:  87\n","Epoch:  88\n","Epoch:  89\n","Epoch:  90\n","Epoch:  91\n","Epoch:  92\n","Epoch:  93\n","Epoch:  94\n","Epoch:  95\n","Epoch:  96\n","Epoch:  97\n","Epoch:  98\n","Epoch:  99\n","Epoch:  100\n","Epoch:  101\n","Epoch:  102\n","Epoch:  103\n","Epoch:  104\n","Epoch:  105\n","Epoch:  106\n","Epoch:  107\n","Epoch:  108\n","Epoch:  109\n","Epoch:  110\n","Epoch:  111\n","Epoch:  112\n","Epoch:  113\n","Epoch:  114\n","Epoch:  115\n","Epoch:  116\n","Epoch:  117\n","Epoch:  118\n","Epoch:  119\n","Epoch:  120\n","Epoch:  121\n","Epoch:  122\n","Epoch:  123\n","Epoch:  124\n","Epoch:  125\n","Epoch:  126\n","Epoch:  127\n","Epoch:  128\n","Epoch:  129\n","Epoch:  130\n","Epoch:  131\n","Epoch:  132\n","Epoch:  133\n","Epoch:  134\n","Epoch:  135\n","Epoch:  136\n","Epoch:  137\n","Epoch:  138\n","Epoch:  139\n","Epoch:  140\n","Epoch:  141\n","Epoch:  142\n","Epoch:  143\n","Epoch:  144\n","Epoch:  145\n","Epoch:  146\n","Epoch:  147\n","Epoch:  148\n","Epoch:  149\n","Epoch:  150\n","Epoch:  151\n","Epoch:  152\n","Epoch:  153\n","Epoch:  154\n","Epoch:  155\n","Epoch:  156\n","Epoch:  157\n","Epoch:  158\n","Epoch:  159\n","Epoch:  160\n","Epoch:  161\n","Epoch:  162\n","Epoch:  163\n","Epoch:  164\n","Epoch:  165\n","Epoch:  166\n","Epoch:  167\n","Epoch:  168\n","Epoch:  169\n","Epoch:  170\n","Epoch:  171\n","Epoch:  172\n","Epoch:  173\n","Epoch:  174\n","Epoch:  175\n","Epoch:  176\n","Epoch:  177\n","Epoch:  178\n","Epoch:  179\n","Epoch:  180\n","Epoch:  181\n","Epoch:  182\n","Epoch:  183\n","Epoch:  184\n","Epoch:  185\n","Epoch:  186\n","Epoch:  187\n","Epoch:  188\n","Epoch:  189\n","Epoch:  190\n","Epoch:  191\n","Epoch:  192\n","Epoch:  193\n","Epoch:  194\n","Epoch:  195\n","Epoch:  196\n","Epoch:  197\n","Epoch:  198\n","Epoch:  199\n","Epoch:  200\n","Epoch:  201\n","Epoch:  202\n","Epoch:  203\n","Epoch:  204\n","Epoch:  205\n","Epoch:  206\n","Epoch:  207\n","Epoch:  208\n","Epoch:  209\n","Epoch:  210\n","Epoch:  211\n","Epoch:  212\n","Epoch:  213\n","Epoch:  214\n","Epoch:  215\n","Epoch:  216\n","Epoch:  217\n","Epoch:  218\n","Epoch:  219\n","Epoch:  220\n","Epoch:  221\n","Epoch:  222\n","Epoch:  223\n","Epoch:  224\n","Epoch:  225\n","Epoch:  226\n","Epoch:  227\n","Epoch:  228\n","Epoch:  229\n","Epoch:  230\n","Epoch:  231\n","Epoch:  232\n","Epoch:  233\n","Epoch:  234\n","Epoch:  235\n","Epoch:  236\n","Epoch:  237\n","Epoch:  238\n","Epoch:  239\n","Epoch:  240\n","Epoch:  241\n","Epoch:  242\n","Epoch:  243\n","Epoch:  244\n","Epoch:  245\n","Epoch:  246\n","Epoch:  247\n","Epoch:  248\n","Epoch:  249\n","Epoch:  250\n","Epoch:  251\n","Epoch:  252\n","Epoch:  253\n","Epoch:  254\n","Epoch:  255\n","Epoch:  256\n","Epoch:  257\n","Epoch:  258\n","Epoch:  259\n","Epoch:  260\n","Epoch:  261\n","Epoch:  262\n","Epoch:  263\n","Epoch:  264\n","Epoch:  265\n","Epoch:  266\n","Epoch:  267\n","Epoch:  268\n","Epoch:  269\n","Epoch:  270\n","Epoch:  271\n","Epoch:  272\n","Epoch:  273\n","Epoch:  274\n","Epoch:  275\n","Epoch:  276\n","Epoch:  277\n","Epoch:  278\n","Epoch:  279\n","Epoch:  280\n","Epoch:  281\n","Epoch:  282\n","Epoch:  283\n","Epoch:  284\n","Epoch:  285\n","Epoch:  286\n","Epoch:  287\n","Epoch:  288\n","Epoch:  289\n","Epoch:  290\n","Epoch:  291\n","Epoch:  292\n","Epoch:  293\n","Epoch:  294\n","Epoch:  295\n","Epoch:  296\n","Epoch:  297\n","Epoch:  298\n","Epoch:  299\n","Epoch:  300\n","Epoch:  301\n","Epoch:  302\n","Epoch:  303\n","Epoch:  304\n","Epoch:  305\n","Epoch:  306\n","Epoch:  307\n","Epoch:  308\n","Epoch:  309\n","Epoch:  310\n","Epoch:  311\n","Epoch:  312\n","Epoch:  313\n","Epoch:  314\n","Epoch:  315\n","Epoch:  316\n","Epoch:  317\n","Epoch:  318\n","Epoch:  319\n","Epoch:  320\n","Epoch:  321\n","Epoch:  322\n","Epoch:  323\n","Epoch:  324\n","Epoch:  325\n","Epoch:  326\n","Epoch:  327\n","Epoch:  328\n","Epoch:  329\n","Epoch:  330\n","Epoch:  331\n","Epoch:  332\n","Epoch:  333\n","Epoch:  334\n","Epoch:  335\n","Epoch:  336\n","Epoch:  337\n","Epoch:  338\n","Epoch:  339\n","Epoch:  340\n","Epoch:  341\n","Epoch:  342\n","Epoch:  343\n","Epoch:  344\n","Epoch:  345\n","Epoch:  346\n","Epoch:  347\n","Epoch:  348\n","Epoch:  349\n","Epoch:  350\n","Epoch:  351\n","Epoch:  352\n","Epoch:  353\n","Epoch:  354\n","Epoch:  355\n","Epoch:  356\n","Epoch:  357\n","Epoch:  358\n","Epoch:  359\n","Epoch:  360\n","Epoch:  361\n","Epoch:  362\n","Epoch:  363\n","Epoch:  364\n","Epoch:  365\n","Epoch:  366\n","Epoch:  367\n","Epoch:  368\n","Epoch:  369\n","Epoch:  370\n","Epoch:  371\n","Epoch:  372\n","Epoch:  373\n","Epoch:  374\n","Epoch:  375\n","Epoch:  376\n","Epoch:  377\n","Epoch:  378\n","Epoch:  379\n","Epoch:  380\n","Epoch:  381\n","Epoch:  382\n","Epoch:  383\n","Epoch:  384\n","Epoch:  385\n","Epoch:  386\n","Epoch:  387\n","Epoch:  388\n","Epoch:  389\n","Epoch:  390\n","Epoch:  391\n","Epoch:  392\n","Epoch:  393\n","Epoch:  394\n","Epoch:  395\n","Epoch:  396\n","Epoch:  397\n","Epoch:  398\n","Epoch:  399\n","Epoch:  400\n","Epoch:  401\n","Epoch:  402\n","Epoch:  403\n","Epoch:  404\n","Epoch:  405\n","Epoch:  406\n","Epoch:  407\n","Epoch:  408\n","Epoch:  409\n","Epoch:  410\n","Epoch:  411\n","Epoch:  412\n","Epoch:  413\n","Epoch:  414\n","Epoch:  415\n","Epoch:  416\n","Epoch:  417\n","Epoch:  418\n","Epoch:  419\n","Epoch:  420\n","Epoch:  421\n","Epoch:  422\n","Epoch:  423\n","Epoch:  424\n","Epoch:  425\n","Epoch:  426\n","Epoch:  427\n","Epoch:  428\n","Epoch:  429\n","Epoch:  430\n","Epoch:  431\n","Epoch:  432\n","Epoch:  433\n","Epoch:  434\n","Epoch:  435\n","Epoch:  436\n","Epoch:  437\n","Epoch:  438\n","Epoch:  439\n","Epoch:  440\n","Epoch:  441\n","Epoch:  442\n","Epoch:  443\n","Epoch:  444\n","Epoch:  445\n","Epoch:  446\n","Epoch:  447\n","Epoch:  448\n","Epoch:  449\n","Epoch:  450\n","Epoch:  451\n","Epoch:  452\n","Epoch:  453\n","Epoch:  454\n","Epoch:  455\n","Epoch:  456\n","Epoch:  457\n","Epoch:  458\n","Epoch:  459\n","Epoch:  460\n","Epoch:  461\n","Epoch:  462\n","Epoch:  463\n","Epoch:  464\n","Epoch:  465\n","Epoch:  466\n","Epoch:  467\n","Epoch:  468\n","Epoch:  469\n","Epoch:  470\n","Epoch:  471\n","Epoch:  472\n","Epoch:  473\n","Epoch:  474\n","Epoch:  475\n","Epoch:  476\n","Epoch:  477\n","Epoch:  478\n","Epoch:  479\n","Epoch:  480\n","Epoch:  481\n","Epoch:  482\n","Epoch:  483\n","Epoch:  484\n","Epoch:  485\n","Epoch:  486\n","Epoch:  487\n","Epoch:  488\n","Epoch:  489\n","Epoch:  490\n","Epoch:  491\n","Epoch:  492\n","Epoch:  493\n","Epoch:  494\n","Epoch:  495\n","Epoch:  496\n","Epoch:  497\n","Epoch:  498\n","Epoch:  499\n","297 29\n","23 296\n","accuracy =  0.9193798449612403 sensitivity =  0.928125 specificity =  0.9107692307692308\n"],"name":"stdout"}]}]}