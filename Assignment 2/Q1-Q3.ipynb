{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q1-Q3.ipynb","provenance":[{"file_id":"1eKijrMozhDKALrL4rdgQa8_tMcqXUiOt","timestamp":1606837791509}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1Px93SHB1j1k"},"source":["# Name: Karthik Suresh\n","# ID: 2018A8PS1229H"]},{"cell_type":"markdown","metadata":{"id":"5Dm4iUWQYI70"},"source":["# Q1 - Non Linear Perceptron"]},{"cell_type":"code","metadata":{"id":"v7F-I_Eb1fLv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606837755984,"user_tz":-330,"elapsed":1302,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}},"outputId":"eb36a659-d998-4a12-c109-c03d00b0e6ab"},"source":["import pandas as pd\n","import numpy as np\n","import math\n","file='/content/drive/MyDrive/2018A8PS1229H_NNFL/Data/data55.xlsx'\n","A=pd.read_excel(file,header=None)\n","d=np.asarray(A,dtype='float')\n","np.random.seed(10)\n","d=np.random.permutation(d)\n","o=np.zeros((len(d),1))\n","o[:,0]=d[:,4]\n","\n","d=np.delete(d,4,1)\n","x1mean=d[:,0].mean()\n","x2mean=d[:,1].mean()\n","x3mean=d[:,2].mean()\n","x4mean=d[:,3].mean()\n","x1std=np.std(d[:,0])\n","x2std=np.std(d[:,1])\n","x3std=np.std(d[:,2])\n","x4std=np.std(d[:,3])\n","#normalising training data\n","d[:,0]=(d[:,0]-d[:,0].mean())/np.std(d[:,0])\n","d[:,1]=(d[:,1]-d[:,1].mean())/np.std(d[:,1])\n","d[:,2]=(d[:,2]-d[:,2].mean())/np.std(d[:,2])\n","d[:,3]=(d[:,3]-d[:,3].mean())/np.std(d[:,3])\n","#hold out cross validation\n","x=d[:70]\n","v=d[70:80]\n","t=d[80:]\n","y=o[:70]\n","vo=o[70:80]\n","to=o[80:]\n","\n","k=100\n","a=0.5\n","\n","#initialising w and b with gaussian distribution\n","w=np.random.normal(0,1,size=(1,4))\n","b=np.random.normal(0,1,size=(1,1))\n","h=np.zeros((len(x),1))\n","lsq=np.zeros((k,1))\n","def sigmoid(sum):\n","  h=1/(1+math.exp(-sum))\n","  return h\n","def cost(h):\n","  j=0.5*np.sum(h-y)**2\n","  return j\n","for i in range(k):\n","  for j in range(len(x)):\n","    sum=np.sum(w@x[j].T)+b\n","    h[j]=sigmoid(sum)\n","    if h[j]>0.5:\n","      yp=1\n","    elif h[j]<0.5 or h[j]==0.5:\n","      yp=0\n","    if y[j]!=yp:\n","      w+=a*y[j]*x[j]\n","      b+=a*y[j]\n","  lsq[i]=cost(h)\n","c=0\n","h=np.zeros((len(v),1))\n","for i in range(len(v)):\n","  sum=np.sum(w@v[i].T)+b\n","  h[i]=sigmoid(sum)\n","  if h[i]>0.5:\n","      yp=1\n","  elif h[i]<0.5 or h[i]==0.5:\n","    yp=0\n","  if vo[i]==yp:\n","    c+=1\n","print(\"the validation accuracy is\",c/len(v))\n","\n","c=0\n","sen=0\n","spe=0\n","h=np.zeros((len(t),1))\n","for i in range(len(t)):\n","  sum=np.sum(w@t[i].T)+b\n","  h[i]=sigmoid(sum)\n","  if h[i]>0.5:\n","      yp=1\n","  elif h[i]<0.5 or h[i]==0.5:\n","    yp=0\n","  if to[i]==yp:\n","    c+=1\n","  if yp==1 and to[i]==1:\n","    sen+=1\n","  if yp==0 and to[i]==0:\n","    spe+=1\n","pos=0\n","for i in range(len(to)):\n","  if to[i]==1:\n","    pos+=1\n","neg=len(to)-pos\n","print(\"the test accuracy is\",c/len(t))\n","print(\"the sensitivity is\",sen/pos)\n","print(\"the specivity is\",spe/neg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the validation accuracy is 1.0\n","the test accuracy is 1.0\n","the sensitivity is 1.0\n","the specivity is 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CBEL3VTN2IIb"},"source":["# Q2 - Kernel Perceptron"]},{"cell_type":"code","metadata":{"id":"kUxpWEBE2Omo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606837668657,"user_tz":-330,"elapsed":1337,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}},"outputId":"19059e83-034c-43b5-f6d8-17f9935bc7b2"},"source":["import pandas as pd\n","import numpy as np\n","import math\n","file='/content/drive/MyDrive/2018A8PS1229H_NNFL/Data/data55.xlsx'\n","A=pd.read_excel(file,header=None)\n","d=np.asarray(A,dtype='float')\n","np.random.seed(20)\n","d=np.random.permutation(d)\n","o=np.zeros((len(d),1))\n","o[:,0]=d[:,4]\n","for i in range(len(o)):\n","  if o[i]==0:\n","    o[i]=-1\n","d=np.delete(d,4,1)\n","#normalising training data\n","d=(d-d.min(axis=0))/(d.max(axis=0)-d.min(axis=0))\n","\n","#hold out cross validation\n","x=d[:70]\n","v=d[70:80]\n","t=d[80:]\n","y=o[:70]\n","vo=o[70:80]\n","to=o[80:]\n","k=100\n","a=np.zeros((len(x),1))\n","def polyker(u,m):\n","  ker=np.zeros((len(m),len(u)))\n","  for i in range(len(m)):\n","    for j in range(len(u)):\n","      ker[i,j]=(1+np.dot(m[i],u[j]))**3\n","\n","  return ker\n","\n","ker=polyker(x,x)\n","\n","for i in range(k):\n","  for j in range(len(x)):\n","    if (np.sum(a*ker))!=0:\n","      yp=np.sign(np.sum(a*ker)*y[j]) #polynomial kernel\n","    else:\n","      yp=-1\n","    if yp!=y[j]:\n","      a[j]+=1\n","c=0\n","ker1=polyker(v,x)\n","\n","for i in range(len(v)):\n","  if np.sum(a*ker1)!=0:\n","    yp=np.sign(np.sum(a*ker1)*vo[i])\n","  else:\n","    yp=-1\n","  if yp==vo[i]:\n","    c+=1\n","print(\"the validation set accuracy is:\",c/len(vo))\n","c=0\n","sen=0\n","spe=0\n","ker2=polyker(t,x)\n","for i in range(len(t)):\n","  \n","  if np.sum(a*ker2)!=0:\n","    yp=np.sign(np.sum(a*ker2)*to[i])\n","  else:\n","    yp=-1\n","  if yp==to[i]:\n","    c+=1\n","  if yp==1 and to[i]==1:\n","    sen+=1\n","  if yp==-1 and to[i]==-1:\n","    spe+=1\n","pos=0\n","for i in range(len(t)):\n","  if to[i]==1:\n","    pos+=1\n","neg=len(to)-pos\n","print(\"the test accuracy is\",c/len(t))\n","print(\"the sensitivity is\",sen/pos)\n","print(\"the specivity is\",spe/neg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the validation set accuracy is: 1.0\n","the test accuracy is 1.0\n","the sensitivity is 1.0\n","the specivity is 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tSIghtx_2eaS"},"source":["# Q3 - Multilayer Perceptron"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ms_ZGamEXOUj","executionInfo":{"status":"ok","timestamp":1606837373403,"user_tz":-330,"elapsed":130591,"user":{"displayName":"KARTHIK SURESH","photoUrl":"","userId":"13511696772299963483"}},"outputId":"d5e1226d-bc79-476e-97fc-68f1814dcc9d"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.io import loadmat\n","\n","mat_contents = loadmat('/content/drive/MyDrive/2018A8PS1229H_NNFL/Data/data5.mat')\n","data = mat_contents['x']\n","np.random.shuffle(data)\n","\n","def init_data():\n","    X = np.array(data[:2148, :-1], dtype = float)\n","    y = np.array(data[:2148, -1], dtype = int)\n","    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n","    return X, y\n","\n","def affine_forward(x, w, b):\n","    z = x.dot(w) + b\n","    cache = (x, w, b)\n","    return z, cache\n","\n","def relu_forward(x):\n","    a = x\n","    a[a<=0] = 0\n","    cache = x\n","    return a, cache\n","\n","def affine_backward(dout, cache):\n","    x, w, b = cache\n","    db = np.sum(dout, axis = 0)\n","    dw = x.T.dot(dout)\n","    dx = dout.dot(w.T)\n","    return dx, dw, db\n","\n","def relu_backward(dout, cache):\n","    x = cache\n","    dx = None\n","    dx = np.ones(x.shape)\n","    dx[x<=0] = 0\n","    dx = dx * dout\n","    return dx\n","\n","class Twonet(object):\n","\n","    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes, std=1e-4):\n","        self.W1 = std * np.random.randn(input_size, hidden_size1)\n","        self.b1 = np.zeros(hidden_size1)\n","        self.W2 = std * np.random.randn(hidden_size1, hidden_size2)\n","        self.b2 = np.zeros(hidden_size2)\n","        self.W3 = std * np.random.randn(hidden_size2, num_classes)\n","        self.b3 = np.zeros(num_classes)\n","\n","    def loss(self, X, y = None, reg = 0.0):\n","        N, D = X.shape\n","        scores = None\n","        z1, af_cache1 = affine_forward(X, self.W1, self.b1)\n","        h1, relu_cache1 = relu_forward(z1)\n","        z2, af_cache2 = affine_forward(h1, self.W2, self.b2)\n","        h2, relu_cache2 = relu_forward(z2)\n","        z3, af_cache3 = affine_forward(h2, self.W3, self.b3)\n","        scores = z3\n","\n","        if y is None:\n","            return scores\n","\n","        loss = None\n","        scores -= scores.max()\n","        scores_exp = np.exp(scores)\n","        correct_scores = scores[range(N), y]\n","        correct_scores_exp = np.exp(correct_scores)\n","        loss = np.sum(-np.log(correct_scores_exp / np.sum(scores_exp, axis = 1))) / N\n","        loss += 0.5 * reg * (np.sum(self.W1 * self.W1) + \\\n","            np.sum(self.W2 * self.W2) + np.sum(self.W3 * self.W3))\n","\n","        num = correct_scores_exp\n","        denom = np.sum(scores_exp, axis = 1)\n","        mask = (np.exp(z3)/denom.reshape(scores.shape[0],1))\n","        mask[range(N),y] = -(denom - num)/denom\n","        mask /= N\n","        dz3 = mask\n","\n","        dh2, dw3, db3 = affine_backward(dz3, af_cache3)\n","        dz2 = relu_backward(dh2, relu_cache2)\n","        dh1, dw2, db2 = affine_backward(dz2, af_cache2)\n","        dz1 = relu_backward(dh1, relu_cache1)\n","        dx, dw1, db1 = affine_backward(dz1, af_cache1)\n","        \n","        dw3 = dw3 + reg * self.W3\n","        dw2 = dw2 + reg * self.W2\n","        dw1 = dw1 + reg * self.W1\n","\n","        wgrad = (dw1, dw2, dw3)\n","        bgrad = (db1, db2, db3)\n","\n","        return loss, wgrad, bgrad\n","\n","    def train(self, X, y, X_val, y_val, alpha = 1e-3, alpha_decay = 0.95,\\\n","         reg = 5e-6, num_iters = 100, batch_size = 200):\n","        num_train = X.shape[0]\n","        iterations_per_epoch = max(num_train / batch_size, 1)\n","        loss_history = []\n","        train_acc_history = []\n","        val_acc_history = []\n","\n","        for it in range(num_iters):\n","\n","            ind = np.random.choice(num_train, batch_size)\n","            X_batch = X[ind,:]\n","            y_batch = y[ind]\n","            \n","            loss, wgrad, bgrad = self.loss(X_batch, y = y_batch, reg = reg)\n","            loss_history.append(loss)\n","\n","            dw1, dw2, dw3 = wgrad\n","            db1, db2, db3 = bgrad\n","\n","            self.W1 -= alpha * dw1\n","            self.W2 -= alpha * dw2\n","            self.W3 -= alpha * dw3\n","            self.b1 -= alpha * db1\n","            self.b2 -= alpha * db2\n","            self.b3 -= alpha * db3\n","\n","\n","            if it % 100 == 0:\n","                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","\n","            if it % iterations_per_epoch == 0:\n","                train_acc = (self.predict(X_batch) == y_batch).mean()\n","                val_acc = (self.predict(X_val) == y_val).mean()\n","                train_acc_history.append(train_acc)\n","                val_acc_history.append(val_acc)\n","\n","                alpha *= alpha_decay\n","\n","\n","        return {'loss_history' : loss_history, 'train_acc_history' : \\\n","            train_acc_history, 'val_acc_history' : val_acc_history}\n","\n","\n","    def predict(self, X):\n","        y_pred = np.argmax(self.loss(X), axis = 1)\n","        return y_pred\n","\n","\n","\n","input_size = 72\n","hidden_size1 = 30\n","hidden_size2 = 30\n","num_classes = 2\n","num_inputs = 1790\n","split_num = 358\n","std = 0.1\n","alpha = 0.3\n","batch_size = 1024\n","reg = 1e-2\n","num_iters = 5000\n","\n","X_tot, y_tot = init_data()\n","\n","train_acc , val_acc = 0, 0\n","losses = np.empty((5, num_iters))\n","val_accs = []\n","train_accs = []\n","\n","for k in range(5):\n","    \n","    X = X_tot[0 : 1790]\n","    y = y_tot[0 : 1790]\n","    X_val = X_tot[1790 :]\n","    y_val = y_tot[1790 :]\n","    \n","    Net = Twonet(input_size, hidden_size1, hidden_size2, num_classes, std)\n","    print(\"Validation fold : \" , k + 1)\n","    stats = Net.train(X, y, X_val, y_val, num_iters = num_iters,\\\n","         alpha = alpha, batch_size = batch_size, reg = 0.0)\n","    losses[k] = np.asarray(stats['loss_history'])\n","    val_accs = np.asarray(stats['val_acc_history'])\n","    train_accs = np.asarray(stats['train_acc_history'])\n","    train_acc += train_accs\n","    val_acc += val_accs\n","\n","\n","    X_tot[: split_num] = X_val\n","    X_tot[split_num : ] = X\n","    y_tot[: split_num] = y_val\n","    y_tot[split_num : ] = y\n","\n","train_acc /= 5\n","val_acc /= 5\n","\n","print(train_acc[-1], val_acc[-1])\n","loss_hist = np.mean(losses, axis = 0)\n","\n","plt.subplot(2, 1, 1)\n","plt.plot(loss_hist)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.xlabel('Epoch')\n","plt.ylabel('Classification accuracy')\n","plt.tight_layout\n","plt.show()\n","\n","y_pred = Net.predict(X_val)\n","TP, TN, FP, FN = 0, 0, 0, 0\n","for i in range(len(y_val)):\n","    if y_pred[i] == 0 and  y_val[i] == 0:\n","        TN += 1\n","    elif y_pred[i] == 1 and  y_val[i] == 0:\n","        FP += 1\n","    elif y_pred[i] == 0 and  y_val[i] == 1:\n","        FN += 1\n","    elif y_pred[i] == 1 and  y_val[i] == 1:\n","        TP += 1\n","\n","print(TP, FP)\n","print(FN, TN)\n","\n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity,\\\n","     \"specificity = \", specificity)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Validation fold :  1\n","iteration 0 / 5000: loss 0.697717\n","iteration 100 / 5000: loss 0.116323\n","iteration 200 / 5000: loss 0.036983\n","iteration 300 / 5000: loss 0.016174\n","iteration 400 / 5000: loss 0.006854\n","iteration 500 / 5000: loss 0.005018\n","iteration 600 / 5000: loss 0.003187\n","iteration 700 / 5000: loss 0.002067\n","iteration 800 / 5000: loss 0.001980\n","iteration 900 / 5000: loss 0.001393\n","iteration 1000 / 5000: loss 0.001374\n","iteration 1100 / 5000: loss 0.001076\n","iteration 1200 / 5000: loss 0.000760\n","iteration 1300 / 5000: loss 0.000667\n","iteration 1400 / 5000: loss 0.000532\n","iteration 1500 / 5000: loss 0.000603\n","iteration 1600 / 5000: loss 0.000478\n","iteration 1700 / 5000: loss 0.000531\n","iteration 1800 / 5000: loss 0.000355\n","iteration 1900 / 5000: loss 0.000400\n","iteration 2000 / 5000: loss 0.000363\n","iteration 2100 / 5000: loss 0.000330\n","iteration 2200 / 5000: loss 0.000356\n","iteration 2300 / 5000: loss 0.000357\n","iteration 2400 / 5000: loss 0.000315\n","iteration 2500 / 5000: loss 0.000295\n","iteration 2600 / 5000: loss 0.000262\n","iteration 2700 / 5000: loss 0.000276\n","iteration 2800 / 5000: loss 0.000252\n","iteration 2900 / 5000: loss 0.000239\n","iteration 3000 / 5000: loss 0.000215\n","iteration 3100 / 5000: loss 0.000202\n","iteration 3200 / 5000: loss 0.000192\n","iteration 3300 / 5000: loss 0.000150\n","iteration 3400 / 5000: loss 0.000199\n","iteration 3500 / 5000: loss 0.000167\n","iteration 3600 / 5000: loss 0.000214\n","iteration 3700 / 5000: loss 0.000181\n","iteration 3800 / 5000: loss 0.000142\n","iteration 3900 / 5000: loss 0.000141\n","iteration 4000 / 5000: loss 0.000163\n","iteration 4100 / 5000: loss 0.000184\n","iteration 4200 / 5000: loss 0.000137\n","iteration 4300 / 5000: loss 0.000170\n","iteration 4400 / 5000: loss 0.000162\n","iteration 4500 / 5000: loss 0.000134\n","iteration 4600 / 5000: loss 0.000155\n","iteration 4700 / 5000: loss 0.000132\n","iteration 4800 / 5000: loss 0.000105\n","iteration 4900 / 5000: loss 0.000135\n","Validation fold :  2\n","iteration 0 / 5000: loss 0.711235\n","iteration 100 / 5000: loss 0.093269\n","iteration 200 / 5000: loss 0.024299\n","iteration 300 / 5000: loss 0.017669\n","iteration 400 / 5000: loss 0.006279\n","iteration 500 / 5000: loss 0.003431\n","iteration 600 / 5000: loss 0.002510\n","iteration 700 / 5000: loss 0.001896\n","iteration 800 / 5000: loss 0.001407\n","iteration 900 / 5000: loss 0.001149\n","iteration 1000 / 5000: loss 0.001138\n","iteration 1100 / 5000: loss 0.000861\n","iteration 1200 / 5000: loss 0.000770\n","iteration 1300 / 5000: loss 0.000562\n","iteration 1400 / 5000: loss 0.000743\n","iteration 1500 / 5000: loss 0.000502\n","iteration 1600 / 5000: loss 0.000497\n","iteration 1700 / 5000: loss 0.000435\n","iteration 1800 / 5000: loss 0.000357\n","iteration 1900 / 5000: loss 0.000453\n","iteration 2000 / 5000: loss 0.000369\n","iteration 2100 / 5000: loss 0.000252\n","iteration 2200 / 5000: loss 0.000365\n","iteration 2300 / 5000: loss 0.000329\n","iteration 2400 / 5000: loss 0.000308\n","iteration 2500 / 5000: loss 0.000288\n","iteration 2600 / 5000: loss 0.000270\n","iteration 2700 / 5000: loss 0.000201\n","iteration 2800 / 5000: loss 0.000214\n","iteration 2900 / 5000: loss 0.000226\n","iteration 3000 / 5000: loss 0.000208\n","iteration 3100 / 5000: loss 0.000218\n","iteration 3200 / 5000: loss 0.000181\n","iteration 3300 / 5000: loss 0.000195\n","iteration 3400 / 5000: loss 0.000160\n","iteration 3500 / 5000: loss 0.000174\n","iteration 3600 / 5000: loss 0.000139\n","iteration 3700 / 5000: loss 0.000162\n","iteration 3800 / 5000: loss 0.000154\n","iteration 3900 / 5000: loss 0.000166\n","iteration 4000 / 5000: loss 0.000153\n","iteration 4100 / 5000: loss 0.000136\n","iteration 4200 / 5000: loss 0.000126\n","iteration 4300 / 5000: loss 0.000122\n","iteration 4400 / 5000: loss 0.000115\n","iteration 4500 / 5000: loss 0.000137\n","iteration 4600 / 5000: loss 0.000112\n","iteration 4700 / 5000: loss 0.000121\n","iteration 4800 / 5000: loss 0.000125\n","iteration 4900 / 5000: loss 0.000110\n","Validation fold :  3\n","iteration 0 / 5000: loss 0.705753\n","iteration 100 / 5000: loss 0.111345\n","iteration 200 / 5000: loss 0.024170\n","iteration 300 / 5000: loss 0.011120\n","iteration 400 / 5000: loss 0.005408\n","iteration 500 / 5000: loss 0.003270\n","iteration 600 / 5000: loss 0.002347\n","iteration 700 / 5000: loss 0.001948\n","iteration 800 / 5000: loss 0.001025\n","iteration 900 / 5000: loss 0.000846\n","iteration 1000 / 5000: loss 0.000796\n","iteration 1100 / 5000: loss 0.000911\n","iteration 1200 / 5000: loss 0.000651\n","iteration 1300 / 5000: loss 0.000507\n","iteration 1400 / 5000: loss 0.000465\n","iteration 1500 / 5000: loss 0.000565\n","iteration 1600 / 5000: loss 0.000492\n","iteration 1700 / 5000: loss 0.000449\n","iteration 1800 / 5000: loss 0.000313\n","iteration 1900 / 5000: loss 0.000363\n","iteration 2000 / 5000: loss 0.000277\n","iteration 2100 / 5000: loss 0.000307\n","iteration 2200 / 5000: loss 0.000290\n","iteration 2300 / 5000: loss 0.000266\n","iteration 2400 / 5000: loss 0.000261\n","iteration 2500 / 5000: loss 0.000278\n","iteration 2600 / 5000: loss 0.000181\n","iteration 2700 / 5000: loss 0.000207\n","iteration 2800 / 5000: loss 0.000203\n","iteration 2900 / 5000: loss 0.000185\n","iteration 3000 / 5000: loss 0.000188\n","iteration 3100 / 5000: loss 0.000193\n","iteration 3200 / 5000: loss 0.000140\n","iteration 3300 / 5000: loss 0.000143\n","iteration 3400 / 5000: loss 0.000188\n","iteration 3500 / 5000: loss 0.000150\n","iteration 3600 / 5000: loss 0.000153\n","iteration 3700 / 5000: loss 0.000185\n","iteration 3800 / 5000: loss 0.000172\n","iteration 3900 / 5000: loss 0.000154\n","iteration 4000 / 5000: loss 0.000138\n","iteration 4100 / 5000: loss 0.000132\n","iteration 4200 / 5000: loss 0.000129\n","iteration 4300 / 5000: loss 0.000114\n","iteration 4400 / 5000: loss 0.000117\n","iteration 4500 / 5000: loss 0.000114\n","iteration 4600 / 5000: loss 0.000135\n","iteration 4700 / 5000: loss 0.000117\n","iteration 4800 / 5000: loss 0.000104\n","iteration 4900 / 5000: loss 0.000108\n","Validation fold :  4\n","iteration 0 / 5000: loss 0.688772\n","iteration 100 / 5000: loss 0.100121\n","iteration 200 / 5000: loss 0.026918\n","iteration 300 / 5000: loss 0.010379\n","iteration 400 / 5000: loss 0.004272\n","iteration 500 / 5000: loss 0.003518\n","iteration 600 / 5000: loss 0.002580\n","iteration 700 / 5000: loss 0.001597\n","iteration 800 / 5000: loss 0.001291\n","iteration 900 / 5000: loss 0.001056\n","iteration 1000 / 5000: loss 0.001049\n","iteration 1100 / 5000: loss 0.000681\n","iteration 1200 / 5000: loss 0.000760\n","iteration 1300 / 5000: loss 0.000732\n","iteration 1400 / 5000: loss 0.000578\n","iteration 1500 / 5000: loss 0.000521\n","iteration 1600 / 5000: loss 0.000453\n","iteration 1700 / 5000: loss 0.000391\n","iteration 1800 / 5000: loss 0.000430\n","iteration 1900 / 5000: loss 0.000320\n","iteration 2000 / 5000: loss 0.000287\n","iteration 2100 / 5000: loss 0.000302\n","iteration 2200 / 5000: loss 0.000270\n","iteration 2300 / 5000: loss 0.000285\n","iteration 2400 / 5000: loss 0.000244\n","iteration 2500 / 5000: loss 0.000245\n","iteration 2600 / 5000: loss 0.000295\n","iteration 2700 / 5000: loss 0.000203\n","iteration 2800 / 5000: loss 0.000246\n","iteration 2900 / 5000: loss 0.000234\n","iteration 3000 / 5000: loss 0.000173\n","iteration 3100 / 5000: loss 0.000205\n","iteration 3200 / 5000: loss 0.000194\n","iteration 3300 / 5000: loss 0.000151\n","iteration 3400 / 5000: loss 0.000176\n","iteration 3500 / 5000: loss 0.000160\n","iteration 3600 / 5000: loss 0.000193\n","iteration 3700 / 5000: loss 0.000164\n","iteration 3800 / 5000: loss 0.000152\n","iteration 3900 / 5000: loss 0.000144\n","iteration 4000 / 5000: loss 0.000139\n","iteration 4100 / 5000: loss 0.000129\n","iteration 4200 / 5000: loss 0.000161\n","iteration 4300 / 5000: loss 0.000130\n","iteration 4400 / 5000: loss 0.000137\n","iteration 4500 / 5000: loss 0.000153\n","iteration 4600 / 5000: loss 0.000126\n","iteration 4700 / 5000: loss 0.000113\n","iteration 4800 / 5000: loss 0.000112\n","iteration 4900 / 5000: loss 0.000104\n","Validation fold :  5\n","iteration 0 / 5000: loss 0.696893\n","iteration 100 / 5000: loss 0.076602\n","iteration 200 / 5000: loss 0.028827\n","iteration 300 / 5000: loss 0.011575\n","iteration 400 / 5000: loss 0.006716\n","iteration 500 / 5000: loss 0.003417\n","iteration 600 / 5000: loss 0.001837\n","iteration 700 / 5000: loss 0.001849\n","iteration 800 / 5000: loss 0.001240\n","iteration 900 / 5000: loss 0.000982\n","iteration 1000 / 5000: loss 0.000944\n","iteration 1100 / 5000: loss 0.000747\n","iteration 1200 / 5000: loss 0.000710\n","iteration 1300 / 5000: loss 0.000685\n","iteration 1400 / 5000: loss 0.000726\n","iteration 1500 / 5000: loss 0.000518\n","iteration 1600 / 5000: loss 0.000517\n","iteration 1700 / 5000: loss 0.000346\n","iteration 1800 / 5000: loss 0.000365\n","iteration 1900 / 5000: loss 0.000314\n","iteration 2000 / 5000: loss 0.000295\n","iteration 2100 / 5000: loss 0.000312\n","iteration 2200 / 5000: loss 0.000323\n","iteration 2300 / 5000: loss 0.000284\n","iteration 2400 / 5000: loss 0.000249\n","iteration 2500 / 5000: loss 0.000280\n","iteration 2600 / 5000: loss 0.000192\n","iteration 2700 / 5000: loss 0.000215\n","iteration 2800 / 5000: loss 0.000211\n","iteration 2900 / 5000: loss 0.000186\n","iteration 3000 / 5000: loss 0.000219\n","iteration 3100 / 5000: loss 0.000187\n","iteration 3200 / 5000: loss 0.000187\n","iteration 3300 / 5000: loss 0.000192\n","iteration 3400 / 5000: loss 0.000149\n","iteration 3500 / 5000: loss 0.000178\n","iteration 3600 / 5000: loss 0.000158\n","iteration 3700 / 5000: loss 0.000151\n","iteration 3800 / 5000: loss 0.000135\n","iteration 3900 / 5000: loss 0.000182\n","iteration 4000 / 5000: loss 0.000127\n","iteration 4100 / 5000: loss 0.000118\n","iteration 4200 / 5000: loss 0.000135\n","iteration 4300 / 5000: loss 0.000134\n","iteration 4400 / 5000: loss 0.000120\n","iteration 4500 / 5000: loss 0.000112\n","iteration 4600 / 5000: loss 0.000114\n","iteration 4700 / 5000: loss 0.000131\n","iteration 4800 / 5000: loss 0.000111\n","iteration 4900 / 5000: loss 0.000102\n","1.0 0.9519553072625697\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["180 0\n","0 178\n","accuracy =  1.0 sensitivity =  1.0 specificity =  1.0\n"],"name":"stdout"}]}]}